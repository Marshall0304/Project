% Solve an Autoregression Time-Series Problem with a NAR Neural Network
% Script generated by Neural Time Series app
% Created Mon Jun 27 22:45:12 CST 2016
%
% This script assumes this variable is defined:
%
%   inputFile - feedback time series.
trainFile = load('inputShang6_train.txt');
trainFile = trainFile(:,5);
trainT = tonndata(trainFile,false,false);

% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. NTSTOOL falls back to this in low memory situations.
trainFcn = 'trainlm';  % Levenberg-Marquardt

% Create a Nonlinear Autoregressive Network
feedbackDelays = 1:5;
hiddenLayerSize = 10;
net = narnet(feedbackDelays,hiddenLayerSize,'open',trainFcn);

% Choose Feedback Pre/Post-Processing Functions
% Settings for feedback input are automatically applied to feedback output
% For a list of all processing functions type: help nnprocess
net.input.processFcns = {'removeconstantrows','mapminmax'};

% Prepare the Data for Training and Simulation
% The function PREPARETS prepares timeseries data for a particular network,
% shifting time by the minimum amount to fill input states and layer states.
% Using PREPARETS allows you to keep your original time series data unchanged, while
% easily customizing it for networks with differing numbers of delays, with
% open loop or closed loop feedback modes.
[x,xi,ai,t] = preparets(net,{},{},trainT);

% Setup Division of Data for Training, Validation, Testing
% For a list of all data division functions type: help nndivide
net.divideFcn = 'dividerand';  % Divide data randomly
net.divideMode = 'time';  % Divide up every value
net.divideParam.trainRatio = 85/100;
net.divideParam.valRatio = 10/100;
net.divideParam.testRatio = 5/100;


% Choose a Performance Function
% For a list of all performance functions type: help nnperformance
net.performFcn = 'mse';  % Mean squared error

% Choose Plot Functions
% For a list of all plot functions type: help nnplot
net.plotFcns = {'plotperform','plottrainstate','plotresponse', ...
  'ploterrcorr', 'plotinerrcorr'};


% Train the Network
[net,tr] = train(net,x,t,xi,ai);

% Test the Network
y = net(x,xi,ai);
e = gsubtract(t,y);
performance = perform(net,t,y)

% Recalculate Training, Validation and Test Performance
trainTargets = gmultiply(t,tr.trainMask);
valTargets = gmultiply(t,tr.valMask);
testTargets = gmultiply(t,tr.testMask);
trainPerformance = perform(net,trainTargets,y)
valPerformance = perform(net,valTargets,y)
testPerformance = perform(net,testTargets,y)

% % View the Network
% view(net)
% 
% % Plots
% % Uncomment these lines to enable various plots.
% %figure, plotperform(tr)
% %figure, plottrainstate(tr)
% %figure, plotresponse(t,y)
% %figure, ploterrcorr(e)
% %figure, plotinerrcorr(x,e)
% 
% % Closed Loop Network
% % Use this network to do multi-step prediction.
% % The function CLOSELOOP replaces the feedback input with a direct
% % connection from the outout layer.
% netc = closeloop(net);
% [xc,xic,aic,tc] = preparets(netc,{},{},trainT);
% yc = netc(xc,xic,aic);
% perfc = perform(net,tc,yc)
% % Multi-step Prediction
% % Sometimes it is useful to simulate a network in open-loop form for as
% % long as there is known data trainT, and then switch to closed-loop to perform
% % multistep prediction. Here The open-loop network is simulated on the known
% % output series, then the network and its final delay states are converted
% % to closed-loop form to produce predictions for 5 more timesteps.
% [x1,xio,aio,t] = preparets(net,{},{},trainT);
% [y1,xfo,afo] = net(x1,xio,aio);
% [netc,xic,aic] = closeloop(net,xfo,afo);
% [y2,xfc,afc] = netc(cell(0,5),xic,aic);
% % Further predictions can be made by continuing simulation starting with
% % the final input and layer delay states, xfc and afc.
% 
% 
% % Step-Ahead Prediction Network
% % For some applications it helps to get the prediction a timestep early.
% % The original network returns predicted y(t+1) at the same time it is given y(t+1).
% % For some applications such as decision making, it would help to have predicted
% % y(t+1) once y(t) is available, but before the actual y(t+1) occurs.
% % The network can be made to return its output a timestep early by removing one delay
% % so that its minimal tap delay is now 0 instead of 1.  The new network returns the
% % same outputs as the original network, but outputs are shifted left one timestep.
% nets = removedelay(net);
% [xs,xis,ais,ts] = preparets(nets,{},{},trainT);
% ys = nets(xs,xis,ais);
% stepAheadPerformance = perform(net,ts,ys)
% 
% % Deployment
% % Change the (false) values to (true) to enable the following code blocks.
% % See the help for each generation function for more information.
% if (false)
%   % Generate MATLAB function for neural network for application deployment
%   % in MATLAB scripts or with MATLAB Compiler and Builder tools, or simply
%   % to examine the calculations your trained neural network performs.
%   genFunction(net,'myNeuralNetworkFunction');
%   y = myNeuralNetworkFunction(x,xi,ai);
% end
% if (false)
%   % Generate a matrix-only MATLAB function for neural network code
%   % generation with MATLAB Coder tools.
%   genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
%   x1 = cell2mat(x(1,:));
%   xi1 = cell2mat(xi(1,:));
%   y = myNeuralNetworkFunction(x1,xi1);
% end
% if (false)
%   % Generate a Simulink diagram for simulation or deployment with.
%   % Simulink Coder tools.
%   gensim(net);
% end

traint = cell2mat(t);
testFile = load('inputShang6_test.txt');
testFile = testFile(:,5);
testT = tonndata(testFile,false,false);

[~,~,ai,~] = preparets(net,{},{},testT);

xi = traint(end-4:end);
xi = num2cell(xi);
preRes = net(testT,xi,ai);
preRes = [y preRes];
preRes = cell2mat(preRes);
testT = cell2mat(testT);
realRes = [traint testT];
plot(realRes,'-b');
hold on;
plot(preRes,'-r');
hold on;

predict = preRes;
count_1 = 0;
count_2 = 0;
count_5 = 0;
count_r = 0;
count_w = 0;
[count_x,count_y]=size(realRes);  
for i=1:(count_y)
    j = i;
    error = (realRes(1,j) - predict(1,i))/realRes(1,j);
    if (error > -0.01 && error < 0.01)
        count_1 = count_1 + 1;
    end
    if (error > -0.02 && error < 0.02)
        count_2 = count_2 + 1;
    end
    if (error > -0.05 && error < 0.05)
        count_5 = count_5 + 1;
    end
    if (i > 7)
        if ((realRes(1,j)-realRes(1,j-7))>0&&(predict(1,i)-realRes(1,j-7))>0 || (realRes(1,j)-realRes(1,j-7))<0&&(predict(1,i)-realRes(1,j-7))<0)
            count_r = count_r + 1;
        else
            count_w = count_w + 1;
        end
    end
    
end  
fid=fopen('result.txt','w');
fprintf(fid,'0.01以内的误差准确率：%f\r\n0.02以内的误差准确率：%f\r\n0.05以内的误差准确率：%f\r\n涨跌准确率：%f\r\n',count_1/count_y,count_2/count_y,count_5/count_y,count_r/(count_r+count_w));  
fclose(fid) ;
